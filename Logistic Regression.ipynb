{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Logistic Regression in Spark", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "\nimport ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'ddP5ODJp-sO3pM7HzxP8fbS0eACIZQBZwaRQMIrdCDkG',\n    'service_id': 'iam-ServiceId-6afcc273-dc9c-4baf-97cc-aa2ce588f115',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_c926ba9e5f744993be30ee0b15b9ade4_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.parquet(cos.url('hmp.parquet', 'advancedmlandsignalprocessing-donotdelete-pr-filqxc7nozcnt1'))\ndf.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+------------+--------------------+\n|  x|  y|  z|       Class|              Source|\n+---+---+---+------------+--------------------+\n| 55| 13| 35|Climb_stairs|Accelerometer-201...|\n| 12| 35| 36|Climb_stairs|Accelerometer-201...|\n| 13| 34| 35|Climb_stairs|Accelerometer-201...|\n| 12| 36| 36|Climb_stairs|Accelerometer-201...|\n| 12| 36| 36|Climb_stairs|Accelerometer-201...|\n| 12| 36| 35|Climb_stairs|Accelerometer-201...|\n| 14| 36| 34|Climb_stairs|Accelerometer-201...|\n| 12| 36| 35|Climb_stairs|Accelerometer-201...|\n| 11| 36| 35|Climb_stairs|Accelerometer-201...|\n| 12| 35| 36|Climb_stairs|Accelerometer-201...|\n| 11| 34| 35|Climb_stairs|Accelerometer-201...|\n| 12| 35| 37|Climb_stairs|Accelerometer-201...|\n| 13| 35| 37|Climb_stairs|Accelerometer-201...|\n| 10| 35| 36|Climb_stairs|Accelerometer-201...|\n|  8| 35| 38|Climb_stairs|Accelerometer-201...|\n|  9| 37| 38|Climb_stairs|Accelerometer-201...|\n| 13| 35| 37|Climb_stairs|Accelerometer-201...|\n| 13| 36| 37|Climb_stairs|Accelerometer-201...|\n| 13| 35| 37|Climb_stairs|Accelerometer-201...|\n| 14| 36| 37|Climb_stairs|Accelerometer-201...|\n+---+---+---+------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "# split the data frame into train and test sample\nsplits = df.randomSplit([0.8,0.2])\ndf_train = splits[0]\ndf_test = splits[1]\nprint(df_train.count())\nprint(df_test.count())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "283385\n70890\n"
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "# build the stages of pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\n\nindexer = StringIndexer(inputCol='Class',outputCol='label')\n#encoder = OneHotEncoder(inputCol='')\nvectorAssembler = VectorAssembler(inputCols = ['x','y','z'],outputCol = 'features')\nnormalizer = Normalizer(inputCol = 'features',outputCol='features_norm')\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 11
        }, 
        {
            "source": "# build the pipeline\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, lr])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 13
        }, 
        {
            "source": "# fit the pipeline to train data\nmodel = pipeline.fit(df_train)\n\n# predict using transform\nprediction = model.transform(df_train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 14
        }, 
        {
            "source": "# evaluate the performance of model (accuracy)\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')\n\nevaluator.evaluate(prediction)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0.12926583975863226"
                    }, 
                    "execution_count": 15, 
                    "metadata": {}
                }
            ], 
            "execution_count": 15
        }, 
        {
            "source": "# evaluate on test dataframe\nmodel_test = pipeline.fit(df_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 16
        }, 
        {
            "source": "prediction_test = model.transform(df_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 17
        }, 
        {
            "source": "evaluator.evaluate(prediction_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0.1293412328960361"
                    }, 
                    "execution_count": 18, 
                    "metadata": {}
                }
            ], 
            "execution_count": 18
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}